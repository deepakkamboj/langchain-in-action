{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<!-- filepath: e:\\hack\\langchain-in-action\\notebooks\\chapter01\\ch01_02_debugging_demo.ipynb -->\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Chapter 1: Agent Debugging and Observability\n",
    "\n",
    "**Purpose:** Learn to debug and monitor LangChain agents using built-in tools and custom logging  \n",
    "**Prerequisites:** Completed ch01_01_first_agent.ipynb  \n",
    "**Duration:** 20-30 minutes  \n",
    "**Key Concepts:** LangSmith tracing, custom logging, performance monitoring, error handling\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to effectively debug, monitor, and optimize LangChain agents using both built-in observability tools and custom logging frameworks.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Enhanced Setup with Debugging Tools\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('../../codes/shared')\n",
    "sys.path.append('../../codes/chapter01')\n",
    "\n",
    "# Standard imports\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom debugging utilities\n",
    "from shared.logging_utils import AgentLogger, error_handling_context, monitor_performance\n",
    "from shared.config import config\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîç Agent Debugging & Observability Setup Complete\")\n",
    "print(f\"üìä LangSmith Tracing: {'Enabled' if config.langchain_tracing_v2 else 'Disabled'}\")\n",
    "print(f\"üêõ Debug Mode: {'On' if config.debug else 'Off'}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Part 1: LangSmith Tracing - Built-in Observability\n",
    "\n",
    "LangSmith provides automatic tracing for LangChain agents, giving you deep visibility into:\n",
    "- Agent reasoning steps\n",
    "- Tool execution details  \n",
    "- Token usage and costs\n",
    "- Performance metrics\n",
    "- Error tracking\n",
    "\n",
    "Let's enable and demonstrate LangSmith tracing.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Enable LangSmith Tracing\n",
    "\n",
    "def setup_langsmith_tracing():\n",
    "    \"\"\"Enable LangSmith tracing for comprehensive observability.\"\"\"\n",
    "    \n",
    "    if config.langchain_api_key:\n",
    "        # Enable tracing\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-book-debugging\"\n",
    "        os.environ[\"LANGCHAIN_API_KEY\"] = config.langchain_api_key\n",
    "        \n",
    "        print(\"‚úÖ LangSmith tracing enabled\")\n",
    "        print(f\"üìä Project: langchain-book-debugging\")\n",
    "        print(\"üîó View traces at: https://smith.langchain.com/\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  LangSmith API key not configured\")\n",
    "        print(\"üí° Set LANGCHAIN_API_KEY environment variable to enable tracing\")\n",
    "        return False\n",
    "\n",
    "# Setup tracing\n",
    "tracing_enabled = setup_langsmith_tracing()\n",
    "\n",
    "if tracing_enabled:\n",
    "    print(\"\\nüéØ All agent interactions will now be traced!\")\n",
    "    print(\"   - Reasoning steps visible\")  \n",
    "    print(\"   - Tool executions logged\")\n",
    "    print(\"   - Performance metrics captured\")\n",
    "    print(\"   - Errors automatically tracked\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Create Agent with Tracing\n",
    "\n",
    "@monitor_performance(\"weather_tool_execution\")\n",
    "def get_weather_with_monitoring(city: str) -> str:\n",
    "    \"\"\"Get weather for a city with performance monitoring.\"\"\"\n",
    "    # Simulate API call delay\n",
    "    time.sleep(0.1)\n",
    "    return f\"It's always sunny in {city}! Perfect weather for outdoor activities.\"\n",
    "\n",
    "def create_monitored_agent():\n",
    "    \"\"\"Create agent with monitoring and tracing enabled.\"\"\"\n",
    "    try:\n",
    "        agent = create_agent(\n",
    "            model=\"anthropic:claude-sonnet-4-5\",\n",
    "            tools=[get_weather_with_monitoring],\n",
    "            system_prompt=\"You are a helpful weather assistant who provides detailed forecasts.\",\n",
    "        )\n",
    "        return agent\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating monitored agent: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the monitored agent\n",
    "monitored_agent = create_monitored_agent()\n",
    "\n",
    "if monitored_agent:\n",
    "    print(\"‚úÖ Monitored agent created successfully!\")\n",
    "    print(\"üîç Agent ready with full observability\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not create monitored agent\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Test Agent with Tracing\n",
    "\n",
    "if monitored_agent:\n",
    "    print(\"üß™ Testing Agent with Full Tracing\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Test queries that will generate traces\n",
    "    test_queries = [\n",
    "        \"What's the weather in San Francisco?\",\n",
    "        \"How about the weather in New York City?\", \n",
    "        \"Tell me about London's weather today\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîç Trace {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = monitored_agent.invoke({\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "            })\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Extract response  \n",
    "            if isinstance(response, dict) and 'messages' in response:\n",
    "                agent_response = response['messages'][-1]['content']\n",
    "            else:\n",
    "                agent_response = str(response)\n",
    "                \n",
    "            print(f\"ü§ñ Response: {agent_response[:100]}...\")\n",
    "            print(f\"‚è±Ô∏è  Execution Time: {execution_time:.2f}s\")\n",
    "            \n",
    "            if tracing_enabled:\n",
    "                print(\"üìä Trace captured in LangSmith\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Tracing demonstration complete!\")\n",
    "    if tracing_enabled:\n",
    "        print(\"üîó View detailed traces at: https://smith.langchain.com/\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping tracing tests - agent not available\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Part 2: Custom Agent Logging Framework\n",
    "\n",
    "While LangSmith provides excellent built-in tracing, sometimes you need custom logging for specific debugging scenarios. Let's explore our custom logging framework.\n",
    "\n",
    "Key features:\n",
    "- **Step-by-step logging** of agent reasoning\n",
    "- **Tool execution tracking** with performance metrics\n",
    "- **Error context capture** for debugging failures\n",
    "- **Conversation history** for analysis\n",
    "- **Performance summaries** for optimization\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Custom Agent Logger Demonstration\n",
    "\n",
    "# Initialize custom logger\n",
    "agent_logger = AgentLogger(\"debugging_demo\", log_level=\"DEBUG\")\n",
    "\n",
    "print(\"üéØ Custom Agent Logger Initialized\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate agent interaction with detailed logging\n",
    "def simulate_agent_interaction(query: str, logger: AgentLogger):\n",
    "    \"\"\"Simulate agent interaction with comprehensive logging.\"\"\"\n",
    "    \n",
    "    with error_handling_context(\"agent_interaction\", logger):\n",
    "        # Log user query\n",
    "        logger.log_agent_step(\"USER_QUERY\", query, {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query_length\": len(query),\n",
    "            \"query_type\": \"weather\" if \"weather\" in query.lower() else \"general\"\n",
    "        })\n",
    "        \n",
    "        # Simulate reasoning step\n",
    "        logger.log_agent_step(\"AGENT_REASONING\", \"Analyzing query for weather request\", {\n",
    "            \"reasoning_type\": \"intent_classification\",\n",
    "            \"confidence\": 0.95\n",
    "        })\n",
    "        \n",
    "        # Simulate tool execution\n",
    "        tool_start = time.time()\n",
    "        time.sleep(0.1)  # Simulate API call\n",
    "        tool_duration = time.time() - tool_start\n",
    "        tool_result = f\"Weather data for query: {query}\"\n",
    "        \n",
    "        logger.log_tool_execution(\n",
    "            tool_name=\"get_weather\",\n",
    "            input_data=query,\n",
    "            output=tool_result,\n",
    "            duration=tool_duration,\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "        # Simulate LLM response generation\n",
    "        llm_start = time.time()\n",
    "        time.sleep(0.05)  # Simulate LLM processing\n",
    "        llm_duration = time.time() - llm_start\n",
    "        llm_response = f\"Based on the weather data, it's sunny in the requested location!\"\n",
    "        \n",
    "        logger.log_llm_interaction(\n",
    "            prompt=f\"Generate weather response for: {query}\",\n",
    "            response=llm_response,\n",
    "            duration=llm_duration,\n",
    "            token_usage={\"prompt_tokens\": 50, \"completion_tokens\": 30, \"total_tokens\": 80}\n",
    "        )\n",
    "        \n",
    "        return llm_response\n",
    "\n",
    "# Test custom logging\n",
    "test_queries = [\n",
    "    \"What's the weather in Miami?\",\n",
    "    \"How about Seattle today?\",\n",
    "    \"Is it raining in Portland?\"\n",
    "]\n",
    "\n",
    "print(\"Testing custom logging framework...\")\n",
    "for query in test_queries:\n",
    "    result = simulate_agent_interaction(query, agent_logger)\n",
    "    print(f\"‚úÖ Logged interaction: {query[:30]}...\")\n",
    "\n",
    "print(f\"\\nüìä Custom logging demonstration complete!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Analyze Logged Data\n",
    "\n",
    "print(\"üìà Analyzing Logged Agent Data\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get conversation summary\n",
    "summary = agent_logger.get_conversation_summary()\n",
    "\n",
    "print(f\"üìã Conversation Summary:\")\n",
    "print(f\"   Total steps logged: {summary['total_steps']}\")\n",
    "print(f\"   Tool executions: {summary['tool_executions']}\")\n",
    "print(f\"   LLM interactions: {summary['llm_interactions']}\")\n",
    "print(f\"   Tools used: {', '.join(summary['tools_used'])}\")\n",
    "print(f\"   Total duration: {summary['total_duration_ms']:.0f}ms\")\n",
    "\n",
    "# Get performance metrics\n",
    "performance = agent_logger.get_performance_summary()\n",
    "\n",
    "if performance:\n",
    "    print(f\"\\nüîß Tool Performance Analysis:\")\n",
    "    for tool_name, metrics in performance.items():\n",
    "        print(f\"   {tool_name}:\")\n",
    "        print(f\"     ‚Ä¢ Total calls: {metrics['total_calls']}\")\n",
    "        print(f\"     ‚Ä¢ Success rate: {metrics['success_rate']}%\")\n",
    "        print(f\"     ‚Ä¢ Average duration: {metrics['average_duration_ms']:.1f}ms\")\n",
    "        print(f\"     ‚Ä¢ Total time: {metrics['total_duration_ms']:.1f}ms\")\n",
    "\n",
    "# Show recent log entries\n",
    "print(f\"\\nüìù Recent Log Entries:\")\n",
    "recent_logs = summary['conversation_log'][-3:]  # Last 3 entries\n",
    "for i, log_entry in enumerate(recent_logs, 1):\n",
    "    print(f\"{i}. [{log_entry['step_type']}] {log_entry['content'][:50]}...\")\n",
    "    if log_entry['metadata']:\n",
    "        key_metadata = {k: v for k, v in log_entry['metadata'].items() if k in ['duration_ms', 'success', 'tool_name']}\n",
    "        if key_metadata:\n",
    "            print(f\"   Metadata: {key_metadata}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Part 3: Error Handling and Recovery Patterns\n",
    "\n",
    "Robust agents need comprehensive error handling. Let's explore common failure patterns and recovery strategies.\n",
    "\n",
    "Common agent errors:\n",
    "- **API timeouts** - external service delays\n",
    "- **Tool failures** - invalid inputs or service unavailability  \n",
    "- **Parsing errors** - malformed model responses\n",
    "- **Rate limiting** - API quota exceeded\n",
    "- **Memory issues** - conversation context too long\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Error Handling Demonstration\n",
    "\n",
    "def create_error_prone_tool():\n",
    "    \"\"\"Create a tool that demonstrates various error conditions.\"\"\"\n",
    "    \n",
    "    @monitor_performance(\"error_prone_operation\")\n",
    "    def unreliable_calculator(expression: str) -> str:\n",
    "        \"\"\"Calculator that sometimes fails to demonstrate error handling.\"\"\"\n",
    "        \n",
    "        # Simulate different error conditions\n",
    "        import random\n",
    "        error_chance = random.random()\n",
    "        \n",
    "        if error_chance < 0.2:  # 20% chance of timeout\n",
    "            time.sleep(0.1)\n",
    "            raise TimeoutError(\"Calculator service timeout\")\n",
    "        elif error_chance < 0.3:  # 10% chance of invalid input\n",
    "            raise ValueError(f\"Invalid expression: {expression}\")\n",
    "        elif error_chance < 0.35:  # 5% chance of service error\n",
    "            raise ConnectionError(\"Calculator service unavailable\")\n",
    "        else:\n",
    "            # Success case\n",
    "            try:\n",
    "                result = eval(expression.replace('^', '**'))\n",
    "                return f\"Calculation result: {result}\"\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Mathematical error: {str(e)}\")\n",
    "    \n",
    "    return unreliable_calculator\n",
    "\n",
    "# Create error-prone tool\n",
    "error_tool = create_error_prone_tool()\n",
    "\n",
    "print(\"‚ö†Ô∏è  Error-Prone Tool Created\")\n",
    "print(\"üé≤ Tool has random failure patterns for demonstration\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Test Error Handling\n",
    "\n",
    "def test_error_handling_patterns():\n",
    "    \"\"\"Test various error handling patterns.\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing Error Handling Patterns\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    error_logger = AgentLogger(\"error_handling_demo\")\n",
    "    \n",
    "    test_expressions = [\n",
    "        \"10 + 5\",\n",
    "        \"20 * 3\", \n",
    "        \"100 / 4\",\n",
    "        \"15 - 8\",\n",
    "        \"2 ^ 8\"  # Power operation\n",
    "    ]\n",
    "    \n",
    "    success_count = 0\n",
    "    total_attempts = len(test_expressions)\n",
    "    \n",
    "    for i, expression in enumerate(test_expressions, 1):\n",
    "        print(f\"\\nüîç Test {i}: {expression}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        try:\n",
    "            with error_handling_context(f\"calculation_{i}\", error_logger):\n",
    "                result = error_tool(expression)\n",
    "                print(f\"‚úÖ Success: {result}\")\n",
    "                success_count += 1\n",
    "                \n",
    "        except TimeoutError as e:\n",
    "            print(f\"‚è∞ Timeout Error: {e}\")\n",
    "            error_logger.log_agent_step(\"TIMEOUT_ERROR\", str(e), {\n",
    "                \"error_type\": \"TimeoutError\",\n",
    "                \"expression\": expression,\n",
    "                \"retry_recommended\": True\n",
    "            })\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"üìä Value Error: {e}\")\n",
    "            error_logger.log_agent_step(\"VALUE_ERROR\", str(e), {\n",
    "                \"error_type\": \"ValueError\", \n",
    "                \"expression\": expression,\n",
    "                \"user_input_issue\": True\n",
    "            })\n",
    "            \n",
    "        except ConnectionError as e:\n",
    "            print(f\"üåê Connection Error: {e}\")\n",
    "            error_logger.log_agent_step(\"CONNECTION_ERROR\", str(e), {\n",
    "                \"error_type\": \"ConnectionError\",\n",
    "                \"expression\": expression,\n",
    "                \"service_issue\": True\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected Error: {e}\")\n",
    "            error_logger.log_agent_step(\"UNEXPECTED_ERROR\", str(e), {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"expression\": expression\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    success_rate = (success_count / total_attempts) * 100\n",
    "    print(f\"\\nüìä Error Handling Test Results:\")\n",
    "    print(f\"   Successful operations: {success_count}/{total_attempts}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Errors handled gracefully: {total_attempts - success_count}\")\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    error_summary = error_logger.get_conversation_summary()\n",
    "    error_types = {}\n",
    "    for log_entry in error_summary['conversation_log']:\n",
    "        if 'ERROR' in log_entry['step_type']:\n",
    "            error_type = log_entry['metadata'].get('error_type', 'Unknown')\n",
    "            error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    if error_types:\n",
    "        print(f\"\\nüîß Error Type Analysis:\")\n",
    "        for error_type, count in error_types.items():\n",
    "            print(f\"   {error_type}: {count} occurrences\")\n",
    "    \n",
    "    return error_logger\n",
    "\n",
    "# Run error handling tests\n",
    "error_test_logger = test_error_handling_patterns()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Part 4: Performance Monitoring and Optimization\n",
    "\n",
    "Understanding agent performance is crucial for production deployments. Let's explore performance monitoring techniques and optimization strategies.\n",
    "\n",
    "Key performance metrics:\n",
    "- **Response time** - total interaction duration\n",
    "- **Tool execution time** - external service latency\n",
    "- **Token usage** - LLM costs and efficiency\n",
    "- **Success rates** - reliability metrics  \n",
    "- **Memory usage** - conversation context size\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Performance Monitoring Demo\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Advanced performance monitoring for LangChain agents.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'total_interactions': 0,\n",
    "            'total_duration': 0,\n",
    "            'tool_calls': 0,\n",
    "            'tool_duration': 0,\n",
    "            'llm_calls': 0,\n",
    "            'llm_duration': 0,\n",
    "            'errors': 0,\n",
    "            'token_usage': {'prompt': 0, 'completion': 0, 'total': 0}\n",
    "        }\n",
    "        self.response_times = []\n",
    "    \n",
    "    def record_interaction(self, duration: float, tool_calls: int = 0, \n",
    "                         tool_duration: float = 0, tokens: dict = None):\n",
    "        \"\"\"Record interaction metrics.\"\"\"\n",
    "        self.metrics['total_interactions'] += 1\n",
    "        self.metrics['total_duration'] += duration\n",
    "        self.metrics['tool_calls'] += tool_calls\n",
    "        self.metrics['tool_duration'] += tool_duration\n",
    "        self.response_times.append(duration)\n",
    "        \n",
    "        if tokens:\n",
    "            for key in ['prompt', 'completion', 'total']:\n",
    "                if key in tokens:\n",
    "                    self.metrics['token_usage'][key] += tokens[key]\n",
    "    \n",
    "    def get_performance_report(self):\n",
    "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
    "        if self.metrics['total_interactions'] == 0:\n",
    "            return \"No interactions recorded\"\n",
    "        \n",
    "        avg_response_time = self.metrics['total_duration'] / self.metrics['total_interactions']\n",
    "        avg_tool_time = self.metrics['tool_duration'] / max(self.metrics['tool_calls'], 1)\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        sorted_times = sorted(self.response_times)\n",
    "        n = len(sorted_times)\n",
    "        p50 = sorted_times[n//2] if n > 0 else 0\n",
    "        p95 = sorted_times[int(n*0.95)] if n > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_interactions': self.metrics['total_interactions'],\n",
    "            'average_response_time': round(avg_response_time, 3),\n",
    "            'p50_response_time': round(p50, 3),\n",
    "            'p95_response_time': round(p95, 3),\n",
    "            'tool_calls': self.metrics['tool_calls'],\n",
    "            'average_tool_time': round(avg_tool_time, 3),\n",
    "            'total_tokens': self.metrics['token_usage']['total'],\n",
    "            'errors': self.metrics['errors']\n",
    "        }\n",
    "\n",
    "# Initialize performance monitor\n",
    "perf_monitor = PerformanceMonitor()\n",
    "\n",
    "print(\"üìä Performance Monitor Initialized\")\n",
    "print(\"üéØ Ready to track agent performance metrics\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Performance Testing\n",
    "\n",
    "def run_performance_benchmark():\n",
    "    \"\"\"Run performance benchmark on agent operations.\"\"\"\n",
    "    \n",
    "    print(\"üèÉ Running Agent Performance Benchmark\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Simulate various agent interactions\n",
    "    test_scenarios = [\n",
    "        {\"query\": \"Simple weather check\", \"expected_tools\": 1, \"complexity\": \"low\"},\n",
    "        {\"query\": \"Calculate 25 * 4 + 10\", \"expected_tools\": 1, \"complexity\": \"low\"},\n",
    "        {\"query\": \"Weather in 3 cities\", \"expected_tools\": 3, \"complexity\": \"medium\"},\n",
    "        {\"query\": \"Complex analysis task\", \"expected_tools\": 2, \"complexity\": \"high\"},\n",
    "        {\"query\": \"Multi-step calculation\", \"expected_tools\": 2, \"complexity\": \"medium\"}\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nüéØ Scenario {i}: {scenario['query'][:30]}...\")\n",
    "        print(f\"   Complexity: {scenario['complexity']}\")\n",
    "        \n",
    "        # Simulate interaction\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate tool calls based on complexity\n",
    "        tool_calls = scenario['expected_tools']\n",
    "        tool_start = time.time()\n",
    "        time.sleep(0.02 * tool_calls)  # Simulate tool execution\n",
    "        tool_duration = time.time() - tool_start\n",
    "        \n",
    "        # Simulate LLM processing\n",
    "        time.sleep(0.05)  # Simulate LLM response time\n",
    "        \n",
    "        total_duration = time.time() - start_time\n",
    "        \n",
    "        # Record metrics\n",
    "        token_usage = {\n",
    "            'prompt': 50 + (tool_calls * 20),\n",
    "            'completion': 30 + (tool_calls * 10),\n",
    "            'total': 80 + (tool_calls * 30)\n",
    "        }\n",
    "        \n",
    "        perf_monitor.record_interaction(\n",
    "            duration=total_duration,\n",
    "            tool_calls=tool_calls,\n",
    "            tool_duration=tool_duration,\n",
    "            tokens=token_usage\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚è±Ô∏è  Duration: {total_duration:.3f}s\")\n",
    "        print(f\"   üîß Tool calls: {tool_calls}\")\n",
    "        print(f\"   üé´ Tokens: {token_usage['total']}\")\n",
    "    \n",
    "    # Generate performance report\n",
    "    report = perf_monitor.get_performance_report()\n",
    "    \n",
    "    print(f\"\\nüìà Performance Benchmark Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total interactions: {report['total_interactions']}\")\n",
    "    print(f\"Average response time: {report['average_response_time']:.3f}s\")\n",
    "    print(f\"50th percentile: {report['p50_response_time']:.3f}s\")\n",
    "    print(f\"95th percentile: {report['p95_response_time']:.3f}s\")\n",
    "    print(f\"Total tool calls: {report['tool_calls']}\")\n",
    "    print(f\"Average tool time: {report['average_tool_time']:.3f}s\")\n",
    "    print(f\"Total tokens used: {report['total_tokens']}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    print(f\"\\nüéØ Performance Analysis:\")\n",
    "    if report['average_response_time'] < 0.5:\n",
    "        print(\"   ‚úÖ Excellent response times\")\n",
    "    elif report['average_response_time'] < 1.0:\n",
    "        print(\"   ‚úÖ Good response times\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Consider optimization\")\n",
    "    \n",
    "    if report['p95_response_time'] > report['average_response_time'] * 3:\n",
    "        print(\"   ‚ö†Ô∏è  High response time variance detected\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Consistent performance\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Run performance benchmark\n",
    "benchmark_results = run_performance_benchmark()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Part 5: Debugging Best Practices and Troubleshooting\n",
    "\n",
    "Based on our exploration, here are the key debugging and monitoring best practices for LangChain agents:\n",
    "\n",
    "### üîç Observability Stack\n",
    "\n",
    "1. **LangSmith Tracing** (Built-in)\n",
    "   - Automatic trace capture\n",
    "   - Visual reasoning flow\n",
    "   - Token usage tracking\n",
    "   - Error context preservation\n",
    "\n",
    "2. **Custom Logging** (Detailed)\n",
    "   - Step-by-step execution logs\n",
    "   - Performance metrics\n",
    "   - Error categorization\n",
    "   - Conversation history\n",
    "\n",
    "3. **Performance Monitoring** (Proactive)\n",
    "   - Response time tracking\n",
    "   - Tool execution analysis\n",
    "   - Resource usage monitoring\n",
    "   - Success rate metrics\n",
    "\n",
    "### üõ†Ô∏è Debugging Workflow\n",
    "\n",
    "1. **Enable Tracing** - Start with LangSmith for overview\n",
    "2. **Add Custom Logging** - Detailed debugging for specific issues\n",
    "3. **Monitor Performance** - Track metrics over time\n",
    "4. **Analyze Patterns** - Identify optimization opportunities\n",
    "5. **Implement Fixes** - Apply targeted improvements\n",
    "\n",
    "### ‚ö° Performance Optimization Tips\n",
    "\n",
    "- **Cache tool results** when appropriate\n",
    "- **Batch similar operations** to reduce overhead\n",
    "- **Optimize prompts** to reduce token usage\n",
    "- **Implement timeouts** for external services\n",
    "- **Use structured outputs** to reduce parsing overhead\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Debugging Checklist and Summary\n",
    "\n",
    "def generate_debugging_checklist():\n",
    "    \"\"\"Generate comprehensive debugging checklist for agents.\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"üîç Observability Setup\": [\n",
    "            \"‚úì LangSmith tracing enabled\",\n",
    "            \"‚úì Custom logging configured\", \n",
    "            \"‚úì Performance monitoring active\",\n",
    "            \"‚úì Error handling implemented\"\n",
    "        ],\n",
    "        \"üß™ Testing Strategy\": [\n",
    "            \"‚úì Unit tests for individual tools\",\n",
    "            \"‚úì Integration tests for full flows\",\n",
    "            \"‚úì Error condition testing\",\n",
    "            \"‚úì Performance benchmarking\"\n",
    "        ],\n",
    "        \"‚ö° Performance Optimization\": [\n",
    "            \"‚úì Response time monitoring\",\n",
    "            \"‚úì Token usage tracking\",\n",
    "            \"‚úì Tool execution profiling\", \n",
    "            \"‚úì Memory usage analysis\"\n",
    "        ],\n",
    "        \"üõ°Ô∏è Error Handling\": [\n",
    "            \"‚úì Timeout handling\",\n",
    "            \"‚úì Retry logic implementation\",\n",
    "            \"‚úì Graceful degradation\",\n",
    "            \"‚úì User-friendly error messages\"\n",
    "        ],\n",
    "        \"üìä Monitoring & Alerts\": [\n",
    "            \"‚úì Success rate tracking\",\n",
    "            \"‚úì Performance thresholds\",\n",
    "            \"‚úì Error rate monitoring\",\n",
    "            \"‚úì Cost tracking (tokens/API calls)\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Display debugging checklist\n",
    "checklist = generate_debugging_checklist()\n",
    "\n",
    "print(\"üìã LangChain Agent Debugging Checklist\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for category, items in checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Takeaways:\")\n",
    "print(\"   ‚Ä¢ Always enable tracing for development\")\n",
    "print(\"   ‚Ä¢ Implement comprehensive error handling\")\n",
    "print(\"   ‚Ä¢ Monitor performance metrics continuously\")\n",
    "print(\"   ‚Ä¢ Test error conditions explicitly\")\n",
    "print(\"   ‚Ä¢ Use structured logging for analysis\")\n",
    "\n",
    "print(f\"\\n‚úÖ Debugging and observability setup complete!\")\n",
    "print(\"üöÄ Your agents are now ready for production monitoring\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Summary\n",
    "\n",
    "You've successfully learned to debug and monitor LangChain agents using:\n",
    "\n",
    "### ‚úÖ Observability Tools\n",
    "- **LangSmith Tracing**: Automatic trace capture and visualization\n",
    "- **Custom Logging**: Detailed step-by-step execution tracking  \n",
    "- **Performance Monitoring**: Response time and resource usage analysis\n",
    "- **Error Handling**: Comprehensive error capture and recovery\n",
    "\n",
    "### ‚úÖ Best Practices\n",
    "- Enable tracing during development\n",
    "- Implement structured logging for analysis\n",
    "- Monitor performance metrics continuously\n",
    "- Test error conditions explicitly  \n",
    "- Use graceful error handling and recovery\n",
    "\n",
    "### ‚úÖ Production Readiness\n",
    "- Comprehensive error handling patterns\n",
    "- Performance optimization strategies\n",
    "- Monitoring and alerting setup\n",
    "- Debugging workflow establishment\n",
    "\n",
    "With these tools and techniques, you can confidently deploy and maintain LangChain agents in production environments with full visibility into their behavior and performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Ready to explore advanced architectural patterns in Chapter 2!\n",
    "</VSCode.Cell>\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
